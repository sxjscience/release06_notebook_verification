{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31722886",
   "metadata": {},
   "source": [
    "# AutoMM Detection - Evaluate Pretrained Deformable DETR on COCO Format Dataset\n",
    ":label:`sec_automm_detection_eval_ddetr_coco`\n",
    "\n",
    "In this section, our goal is to evaluate Deformable DETR model on COCO17 dataset in COCO format. \n",
    "Previously we introduced two classic models: :ref:`sec_automm_detection_eval_yolov3_coco` and :ref:`sec_automm_detection_eval_fasterrcnn_coco`.\n",
    "Recent years Transformer models become more and more popular in Computer Vision, and Deformable DEtection TRansformer (Deformable DETR) reached the SOTA performance in detection task.\n",
    "In terms of speed, it's slower than YOLOv3 and Faster-RCNN, but it also has higher performance.\n",
    "\n",
    "To start, let's import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0f4f7",
   "metadata": {},
   "source": [
    "We select the two-stage Deformable DETR with ResNet50 as backbone with bounding box finetune,\n",
    "this model has **15.7 frames per second (FPS)** on single A10e GPU with `batch_size=1`.\n",
    "For details about model selection, see :ref:`sec_automm_detection_select_models`.\n",
    "And we use all the GPUs (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"deformable_detr_twostage_refine_r50_16x2_50e_coco\"\n",
    "num_gpus = -1  # use all GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb6e06",
   "metadata": {},
   "source": [
    "We create the MultiModalPredictor with selected checkpoint name and number of GPUs.\n",
    "We also need to specify the problem_type to `\"object_detection\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571164d",
   "metadata": {},
   "source": [
    "Here we use COCO17 for testing. \n",
    "See other tutorials for \\[Prepare COCO2017], \\[Convert VOC Format Dataset to COCO Format], and \\[Create Custom Dataset].\n",
    "While using COCO dataset, the input is the json annotation file of the dataset split.\n",
    "In this example, `instances_val2017.json` is the annotation file of validation split of COCO17 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"coco17/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1498d",
   "metadata": {},
   "source": [
    "To evaluate the pretrained Deformable DETR model we loaded, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8d6af",
   "metadata": {},
   "source": [
    "And the evaluation results are shown in command line output. The first value `0.463` is mAP in COCO standard, and the second one `0.659` is mAP in VOC standard (or mAP50). For more details about these metrics, see [COCO's evaluation guideline](https://cocodataset.org/#detection-eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac095605",
   "metadata": {},
   "outputs": [],
   "source": [
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.659\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.500\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.298\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.493\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.607\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.358\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.603\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.652\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.451\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.692\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.830\n",
    "time usage: 389.92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cae0f",
   "metadata": {},
   "source": [
    "Deformable DETR has best performance but takes more time and (GPU memory) space. \n",
    "If there is a restriction in time or space, \n",
    "see :ref:`sec_automm_detection_eval_fasterrcnn_coco` or :ref:`sec_automm_detection_eval_yolov3_coco`.\n",
    "You can also see other tutorials for \\[Fast Finetune on COCO format data] or \\[Inference on COCO format data (with Visualization)].\n",
    "\n",
    "### Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/awslabs/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "### Customization\n",
    "To learn how to customize AutoMM, please refer to :ref:`sec_automm_customization`.\n",
    "\n",
    "### Citation\n",
    "```\n",
    "@inproceedings{\n",
    "zhu2021deformable,\n",
    "title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},\n",
    "author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\n",
    "booktitle={International Conference on Learning Representations},\n",
    "year={2021},\n",
    "url={https://openreview.net/forum?id=gZ9hCDWe6ke}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
