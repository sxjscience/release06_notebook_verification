{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c38f95",
   "metadata": {},
   "source": [
    "# AutoMM Detection - Inference with Pretrained VFNet on Tiny Motorbike Dataset\n",
    ":label:`sec_automm_detection_infer_tiny_motorbike`\n",
    "\n",
    "In this section, we show an quick-start example to run inference on a small dataset (Tiny Motorbick) that is in COCO Format. \n",
    "The model we use is the VFNet pretrained on COCO dataset.\n",
    "\n",
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebe49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from autogluon.core.utils.loaders import load_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efe117",
   "metadata": {},
   "source": [
    "The data file stored in the cloud is located at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = \"s3://automl-mm-bench/object_detection_dataset/tiny_motorbike_coco.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4a2b2",
   "metadata": {},
   "source": [
    "Now let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d338afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = \"./tiny_motorbike_coco\"  # specify a target download dir to store this dataset\n",
    "\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
    "data_dir = os.path.join(download_dir, \"tiny_motorbike\")\n",
    "train_path = os.path.join(data_dir, \"Annotations\", \"train_cocoformat.json\")\n",
    "test_path = os.path.join(data_dir, \"Annotations\", \"test_cocoformat.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875358f",
   "metadata": {},
   "source": [
    "## Creating the `MultiModalPredictor`\n",
    "To start, import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81620c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abd569",
   "metadata": {},
   "source": [
    "### Use a pretrained model\n",
    "You can download a pretrained model and construct a predictor with it. \n",
    "In this example, we use the VFNet with ResNext as backbone and Feature Pyramid Network (FPN) as neck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
    "num_gpus = 1  # set to -1 to use all GPUs if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9c367",
   "metadata": {},
   "source": [
    "You can also use other model by setting `checkpoint_name` to other names. \n",
    "Please refer to :ref: `selecting_models` for details about model selection.\n",
    "\n",
    "As before, we create the MultiModalPredictor with selected checkpoint name and number of GPUs.\n",
    "We also need to specify the `problem_type` to `\"object_detection\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b95d8",
   "metadata": {},
   "source": [
    "### Use a finetuned model\n",
    "You can also use a previously trained/finetuned predictor to run inference with.\n",
    "First specify the predictor path, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"./AutogluonModels/ag-20221104_185342\"  # replace this with path to your desired predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5dcbf",
   "metadata": {},
   "source": [
    "Then load the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor.load(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615edd84",
   "metadata": {},
   "source": [
    "## Setting up data\n",
    "\n",
    "For COCO format data, we need to provide the path for the data split used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./tiny_motorbike_coco/tiny_motorbike/Annotations/test_cocoformat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756ec60",
   "metadata": {},
   "source": [
    "## Running inference\n",
    "To run inference, perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02665573",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictor.predict(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded9f9a",
   "metadata": {},
   "source": [
    "By default, the `predictor.predict` does not save the detection results into a file.\n",
    "\n",
    "To run inference and save results, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictor.predict(test_path, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99832f6a",
   "metadata": {},
   "source": [
    "Currently, we convert the results to a pandas `DataFrame` and save into a `.txt` file. \n",
    "The `.txt` file has two columns, `image` and `bboxes`, where\n",
    "- in `image`, each row contains the image path\n",
    "- in `bboxes`, each row is a list of dictionaries, each one representing a bounding box: \n",
    "  - `{\"class\": <predicted_class_name>, \"bbox\": [x1, y1, x2, y2], \"score\": <confidence_score>}` \n",
    "\n",
    "## Reading results\n",
    "By default, the returned value `pred` is a `list` and has the following dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_images, num_total_classes, num_detections_per_class, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b02f94",
   "metadata": {},
   "source": [
    "where \n",
    "- `num_images` is the number of images used to run inference on. \n",
    "- `num_total_classes` is the total number of classes depending on the model specified in `predictor`. In this example, `num_total_classes = 80` for the VFNet model pretrained on COCO dataset. To get all available classes in the predictor, run `predictor.get_predictor_classes()`\n",
    "- `num_detections_per_class` is the number of detections under each class. Note that this number can vary across different classes.\n",
    "- The last dimension contains the bounding box information, which follows `[x1, y1, x2, y2, score]` format. `x1, y1` are the top left corner of the bounding box, and `x2, y2` are the bottom right corner. `score` is the confidence score of the prediction\n",
    "\n",
    "example code to examine bounding box information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af295aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_classes = predictor.get_predictor_classes()\n",
    "idx2classname = {idx: classname for (idx, classname) in enumerate(detection_classes)}\n",
    "for i, image_pred in enumerate(pred):\n",
    "    print(\"result for image {}\".format(i))\n",
    "    for j, per_cls_bboxes in enumerate(image_pred):\n",
    "        classname = idx2classname[j]\n",
    "        for bbox in per_cls_bboxes:\n",
    "            ## bbox = [x1, y1, x2, y2, conf_score]\n",
    "            print(\"bbox: {}, class: {}, score: {}\".format(bbox[:4], classname, bbox[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473da73f",
   "metadata": {},
   "source": [
    "If you prefer to get the results in `pd.DataFrame` format, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45446be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = predictor.predict(test_path, as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373b08f",
   "metadata": {},
   "source": [
    "Similar to the `.txt` file, the `pred_df` also has two columns, `image` and `bboxes`, where\n",
    "- in `image`, each row contains the image path\n",
    "- in `bboxes`, each row is a list of dictionaries, each one representing a bounding box: \n",
    "  - `{\"class\": <predicted_class_name>, \"bbox\": [x1, y1, x2, y2], \"score\": <confidence_score>}` \n",
    "\n",
    "## Visualizing Results\n",
    "To run visualizations, ensure that you have `opencv` installed. If you haven't already, install `opencv` by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fe2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261a6aa",
   "metadata": {},
   "source": [
    "To visualize the detection bounding boxes, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal.utils import from_coco_or_voc, visualize_detection\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "conf_threshold = 0.4  # Specify a confidence threshold to filter out unwanted boxes\n",
    "visualization_result_dir = \"./tiny_motorbike_coco/tiny_motorbike/visualizations\"  # Specify a directory to save visualized images.\n",
    "df = from_coco_or_voc(test_path)[:10][[\"image\"]]\n",
    "\n",
    "pred = predictor.predict(df)\n",
    "\n",
    "visualized = visualize_detection(\n",
    "    pred=pred,\n",
    "    detection_classes=predictor.get_predictor_classes(),\n",
    "    conf_threshold=conf_threshold,\n",
    "    visualization_result_dir=visualization_result_dir,\n",
    ")\n",
    "\n",
    "plt.imshow(visualized[0][:, : ,::-1])  # shows the first image with bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a837edb9",
   "metadata": {},
   "source": [
    "Note that we took 10 images to visualize for this example. \n",
    "Please consider your storage situation when deciding the number of images to visualize.\n",
    "\n",
    "The `pred` parameter that `visualize_detection` function takes as input follows the form of a `pandas` `DataFrame`, same as in the `pred_df`. \n",
    "Make sure you have the format when visualizing.\n",
    "### Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/awslabs/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "### Customization\n",
    "To learn how to customize AutoMM, please refer to :ref:`sec_automm_customization`.\n",
    "\n",
    "### Citation\n",
    "```\n",
    "@inproceedings{zhang2021varifocalnet,\n",
    "  title={Varifocalnet: An iou-aware dense object detector},\n",
    "  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and Sunderhauf, Niko},\n",
    "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n",
    "  pages={8514--8523},\n",
    "  year={2021}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
