{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0043836",
   "metadata": {},
   "source": [
    "# AutoMM Detection - Inference with Pretrained VFNet on COCO Dataset\n",
    ":label:`sec_automm_detection_infer_coco`\n",
    "\n",
    "In this section, we show an example to run inference COCO dataset in COCO Format. \n",
    "Different from running evaluation, the purpose is to get detection results for potential down-stream tasks.\n",
    "The model we use is the VFNet pretrained on COCO dataset.\n",
    "\n",
    "## Prepare data\n",
    "For running this tutorial, you should have COCO dataset prepared.\n",
    "If you haven't already, head over to :label:`sec_automm_detection_prepare_coco17` to learn how to get COCO dataset.\n",
    "This tutorial assumes that COCO dataset is under `~/data`, i.e. it is located in `~/data/coco17`\n",
    "\n",
    "## Creating the `MultiModalPredictor`\n",
    "To start, import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6867f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12badc15",
   "metadata": {},
   "source": [
    "### Use a pretrained model\n",
    "You can download a pretrained model and construct a predictor with it. \n",
    "In this example, we use the VFNet with ResNext as backbone and Feature Pyramid Network (FPN) as neck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
    "num_gpus = 1  # set to -1 to use all GPUs if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c8e5c",
   "metadata": {},
   "source": [
    "You can also use other model by setting `checkpoint_name` to other names. \n",
    "Please refer to :ref: `selecting_models` for details about model selection.\n",
    "\n",
    "As before, we create the MultiModalPredictor with selected checkpoint name and number of GPUs.\n",
    "We also need to specify the `problem_type` to `\"object_detection\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299490f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613945a4",
   "metadata": {},
   "source": [
    "### Use a finetuned model\n",
    "You can also use a previously trained/finetuned predictor to run inference with.\n",
    "First specify the predictor path, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cde5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"./AutogluonModels/ag-20221104_185342\"  # replace this with path to your desired predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8299750",
   "metadata": {},
   "source": [
    "Then load the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor.load(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df54fa0",
   "metadata": {},
   "source": [
    "## Setting up data\n",
    "\n",
    "For COCO format data, we need to provide the path for the data split used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"~/data/coco17/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad4474",
   "metadata": {},
   "source": [
    "## Running inference\n",
    "To run inference, perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictor.predict(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fceed76",
   "metadata": {},
   "source": [
    "By default, the `predictor.predict` does not save the detection results into a file.\n",
    "\n",
    "To run inference and save results, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d072600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictor.predict(test_path, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed483a9d",
   "metadata": {},
   "source": [
    "Currently, we convert the results to a pandas `DataFrame` and save into a `.txt` file.\n",
    "The `.txt` file has two columns, `image` and `bboxes`, where\n",
    "- in `image`, each row contains the image path\n",
    "- in `bboxes`, each row is a list of dictionaries, each one representing a bounding box: \n",
    "  - `{\"class\": <predicted_class_name>, \"bbox\": [x1, y1, x2, y2], \"score\": <confidence_score>}`\n",
    "\n",
    "## Reading results\n",
    "By default, the returned value `pred` is a `list` and has the following dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fa6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_images, num_total_classes, num_detections_per_class, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ba1b",
   "metadata": {},
   "source": [
    "where \n",
    "- `num_images` is the number of images used to run inference on. \n",
    "- `num_total_classes` is the total number of classes depending on the model specified in `predictor`. In this example, `num_total_classes = 80` for the VFNet model pretrained on COCO dataset. To get all available classes in the predictor, run `predictor.get_predictor_classes()`\n",
    "- `num_detections_per_class` is the number of detections under each class. Note that this number can vary across different classes.\n",
    "- The last dimension contains the bounding box information, which follows `[x1, y1, x2, y2, score]` format. `x1, y1` are the top left corner of the bounding box, and `x2, y2` are the bottom right corner. `score` is the confidence score of the prediction\n",
    "\n",
    "example code to examine bounding box information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_classes = predictor.get_predictor_classes()\n",
    "idx2classname = {idx: classname for (idx, classname) in enumerate(detection_classes)}\n",
    "for i, image_pred in enumerate(pred):\n",
    "    print(\"result for image {}\".format(i))\n",
    "    for j, per_cls_bboxes in enumerate(image_pred):\n",
    "        classname = idx2classname[j]\n",
    "        for bbox in per_cls_bboxes:\n",
    "            ## bbox = [x1, y1, x2, y2, conf_score]\n",
    "            print(\"bbox: {}, class: {}, score: {}\".format(bbox[:4], classname, bbox[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02ce84",
   "metadata": {},
   "source": [
    "If you prefer to get the results in `pd.DataFrame` format, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = predictor.predict(test_path, as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac785a4",
   "metadata": {},
   "source": [
    "Similar to the `.txt` file, the `pred_df` also has two columns, `image` and `bboxes`, where\n",
    "- in `image`, each row contains the image path\n",
    "- in `bboxes`, each row is a list of dictionaries, each one representing a bounding box: \n",
    "  - `{\"class\": <predicted_class_name>, \"bbox\": [x1, y1, x2, y2], \"score\": <confidence_score>}`\n",
    "\n",
    "## Visualizing Results\n",
    "To run visualizations, ensure that you have `opencv` installed. If you haven't already, install `opencv` by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d3a11",
   "metadata": {},
   "source": [
    "To visualize the detection bounding boxes, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f96d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal.utils import from_coco_or_voc, visualize_detection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_threshold = 0.4  # Specify a confidence threshold to filter out unwanted boxes\n",
    "visualization_result_dir = \"~/data/coco17/visualizations\"  # Specify a directory to save visualized images.\n",
    "df = from_coco_or_voc(test_path)[:10][[\"image\"]]\n",
    "\n",
    "pred = predictor.predict(df)\n",
    "\n",
    "visualize_detection(\n",
    "    pred=pred,\n",
    "    detection_classes=predictor.get_predictor_classes(),\n",
    "    conf_threshold=conf_threshold,\n",
    "    visualization_result_dir=visualization_result_dir,\n",
    ")\n",
    "\n",
    "plt.imshow(visualized[0][:, : ,::-1])  # shows the first image with bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62021315",
   "metadata": {},
   "source": [
    "Note that we took 10 images to visualize for this example. \n",
    "Please consider your storage situation when deciding the number of images to visualize. \n",
    "\n",
    "The `pred` parameter that `visualize_detection` function takes as input follows the form of a `pandas` `DataFrame`, same as in the `pred_df`. \n",
    "Make sure you have the format when visualizing. \n",
    "\n",
    "### Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/awslabs/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "### Customization\n",
    "To learn how to customize AutoMM, please refer to :ref:`sec_automm_customization`.\n",
    "\n",
    "### Citation\n",
    "```\n",
    "@inproceedings{zhang2021varifocalnet,\n",
    "  title={Varifocalnet: An iou-aware dense object detector},\n",
    "  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and Sunderhauf, Niko},\n",
    "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n",
    "  pages={8514--8523},\n",
    "  year={2021}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
