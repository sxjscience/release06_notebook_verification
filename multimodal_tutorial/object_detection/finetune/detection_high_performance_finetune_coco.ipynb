{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa43a6b",
   "metadata": {},
   "source": [
    "# AutoMM Detection - High Performance Finetune on COCO Format Dataset\n",
    ":label:`sec_automm_detection_high_performance_finetune_coco`\n",
    "\n",
    "In this section, our goal is to finetune a high performance model on VOC2017 training set, \n",
    "and evaluate it in VOC2007 test set. Both training and test sets are in COCO format.\n",
    "See :ref:`sec_automm_detection_prepare_voc` for how to prepare VOC dataset,\n",
    "and :ref:`sec_automm_detection_convert_to_coco` for how to convert other datasets to COCO format.\n",
    "\n",
    "To start, let's import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe00308",
   "metadata": {},
   "source": [
    "We select the VFNet with ResNeXt-101 as backbone, Feature Pyramid Network (FPN) as neck,\n",
    "and input resolution is 640x640, pretrained on COCO dataset.\n",
    "*(The neck of the object detector refers to the additional layers existing between the backbone and the head. \n",
    "Their role is to collect feature maps from different stages.)*\n",
    "With this setting, it sacrifices training and inference time,\n",
    "and also requires much more GPU memory,\n",
    "but the performance is high. \n",
    "For more model choices, see :label:`sec_automm_detection_selecting_models`.\n",
    "\n",
    "We use `val_metric = map`, i.e., mean average precision in COCO standard as our validation metric.\n",
    "In previous section :ref:`sec_automm_detection_fast_finetune_coco`,\n",
    "we did not specify the validation metric and by default the validation loss is used as validation metric.\n",
    "Using validation loss is much faster but using mean average precision gives the best performance.\n",
    "\n",
    "While using COCO format dataset, the input is the json annotation file of the dataset split.\n",
    "In this example, `voc07_train.json` and `voc07_test.json` \n",
    "are the annotation files of train and test split of VOC2007 dataset.\n",
    "And we use all the GPUs (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae531a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
    "num_gpus = -1  # use all GPUs\n",
    "val_metric = \"map\"\n",
    "\n",
    "train_path = \"./VOCdevkit/VOC2007/Annotations/train_cocoformat.json\" \n",
    "test_path = \"./VOCdevkit/VOC2007/Annotations/test_cocoformat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8347da",
   "metadata": {},
   "source": [
    "We create the MultiModalPredictor with selected checkpoint name, val_metric, and number of GPUs.\n",
    "We need to specify the problem_type to `\"object_detection\"`,\n",
    "and also provide a `sample_data_path` for the predictor to infer the catgories of the dataset.\n",
    "Here we provide the `train_path`, and it also works using any other split of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c60f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "        \"optimization.val_metric\": val_metric,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    "    sample_data_path=train_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd8c3a",
   "metadata": {},
   "source": [
    "If no data sample is available at this point, you can also create the MultiModalPredictor by manually input the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c16f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "        \"optimization.val_metric\": val_metric,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    "    classes=voc_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d063f6",
   "metadata": {},
   "source": [
    "We set the learning rate to be `1e-5` and epoch to be 20 for fast finetuning.\n",
    "Note that we use a two-stage learning rate option during finetuning by default,\n",
    "and the model head will have 100x learning rate.\n",
    "Using a two-stage learning rate with high learning rate only on head layers makes\n",
    "the model converge faster during finetuning. It usually gives better performance as well,\n",
    "especially on small datasets with hundreds or thousands of images.\n",
    "We also set the batch_size to be 2, because this model is too huge to run with larger batch size.\n",
    "For more information about how to tune those hyperparameters,\n",
    "see :ref:`sec_automm_detection_tune_hyperparameters`.\n",
    "We also compute the time of the fit process here for better understanding the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f10eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predictor.fit(\n",
    "    train_path,\n",
    "    hyperparameters={\n",
    "        \"optimization.learning_rate\": 1e-5, # we use two stage and detection head has 100x lr\n",
    "        \"optimization.max_epochs\": 20,\n",
    "        \"env.per_gpu_batch_size\": 1,  # decrease it when model is large\n",
    "    },\n",
    ")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1992d",
   "metadata": {},
   "source": [
    "We run it on a g5dn.12xlarge EC2 machine on AWS,\n",
    "and part of the command outputs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 0:  50%|███████████████████████████████████████████▌                                           | 394/788 [07:42<07:42,  1.17s/it, loss=1.52, v_num=Epoch 0, global step 20: 'val_map' reached 0.61814 (best 0.61814), saving model to '/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558/epoch=0-step=20.ckpt' as top 1                                                                                                                     \n",
    "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████| 788/788 [15:29<00:00,  1.18s/it, loss=0.982, v_num=Epoch 0, global step 41: 'val_map' reached 0.68742 (best 0.68742), saving model to '/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558/epoch=0-step=41.ckpt' as top 1                                                                                                                     \n",
    "Epoch 1:  50%|████████████████████████████████████████████                                            | 394/788 [07:54<07:54,  1.20s/it, loss=0.879, v_numEpoch 1, global step 61: 'val_map' reached 0.70111 (best 0.70111), saving model to '/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558/epoch=1-step=61.ckpt' as top 1                                                                                                                    \n",
    "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████| 788/788 [15:49<00:00,  1.21s/it, loss=0.759, v_num=Epoch 1, global step 82: 'val_map' reached 0.70580 (best 0.70580), saving model to '/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558/epoch=1-step=82.ckpt' as top 1                                                                                                                   \n",
    "Epoch 2:  50%|████████████████████████████████████████████▌                                            | 394/788 [07:47<07:47,  1.19s/it, loss=1.11, v_num=Epoch 2, global step 102: 'val_map' was not in top 1                                                                                                             \n",
    "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████| 788/788 [15:29<00:00,  1.18s/it, loss=0.712, v_num=Epoch 2, global step 123: 'val_map' reached 0.71277 (best 0.71277), saving model to '/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558/epoch=2-step=123.ckpt' as top 1                                                                                                                 \n",
    "Epoch 3:  50%|████████████████████████████████████████████▌                                            | 394/788 [07:38<07:38,  1.16s/it, loss=1.07, v_num=Epoch 3, global step 143: 'val_map' was not in top 1                                                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6912f",
   "metadata": {},
   "source": [
    "Notice that at the end of each progress bar, if the checkpoint at current stage is saved,\n",
    "it prints the model's save path.\n",
    "In this example, it's `/media/code/autogluon/examples/automm/object_detection/AutogluonModels/ag-20221104_051558`.\n",
    "You can also specify the `save_path` like below while creating the MultiModalPredictor.\n",
    "For more information about save and load the model,\n",
    "see :ref:`sec_automm_detection_save_and_load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    save_path=\"./this_is_a_save_path\",\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111f663",
   "metadata": {},
   "source": [
    "Print out the time and we can see that it takes almost 5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This finetuning takes %.2f seconds.\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a51c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "This finetuning takes 17779.09 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b163c0",
   "metadata": {},
   "source": [
    "It does take a lot of time but let's look at its performance.\n",
    "To evaluate the model we just trained, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30450785",
   "metadata": {},
   "source": [
    "And the evaluation results are shown in command line output. The first value `0.740` is mAP in COCO standard, and the second one `0.932` is mAP in VOC standard (or mAP50). For more details about these metrics, see [COCO's evaluation guideline](https://cocodataset.org/#detection-eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.740                                                                               \n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.932                                                                               \n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.819                                                                               \n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.483                                                                               \n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.617                                                                               \n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.792                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.569                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.811                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.827                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.603                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.754                                                                               \n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.866  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89f9f1",
   "metadata": {},
   "source": [
    "Under this high performance finetune setting, it took 5 hours but reached `mAP50 = 0.932` on VOC!\n",
    "For how to finetune faster,\n",
    "see :ref:`sec_automm_detection_fast_finetune_coco`, where we finetuned a YOLOv3 model with\n",
    "100 seconds and reached `mAP50 = 0.755` on VOC.\n",
    "\n",
    "### Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/awslabs/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "### Customization\n",
    "To learn how to customize AutoMM, please refer to :ref:`sec_automm_customization`.\n",
    "\n",
    "### Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@article{DBLP:journals/corr/abs-2008-13367,\n",
    "  author    = {Haoyang Zhang and\n",
    "               Ying Wang and\n",
    "               Feras Dayoub and\n",
    "               Niko S{\\\"{u}}nderhauf},\n",
    "  title     = {VarifocalNet: An IoU-aware Dense Object Detector},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/2008.13367},\n",
    "  year      = {2020},\n",
    "  url       = {https://arxiv.org/abs/2008.13367},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {2008.13367},\n",
    "  timestamp = {Wed, 16 Sep 2020 11:20:03 +0200},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-13367.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
